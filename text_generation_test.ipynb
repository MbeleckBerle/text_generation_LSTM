{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 23:03:31.881066: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-11 23:03:31.893414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-11 23:03:31.904715: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-11 23:03:31.908088: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-11 23:03:31.916429: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 23:03:33.000720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 696 unique bigrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728702214.741173   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728702214.780822   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728702214.780894   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728702214.784750   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728702214.784807   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728702214.784825   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728702214.881092   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728702214.881218   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-11 23:03:34.881230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728702214.881274   53389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-11 23:03:34.881289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:340: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(name=name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (x): [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n",
      "Target (y): [[b'th' b' d' b'id' ... b'ei' b'th' b'er']\n",
      " [b'er' b'e ' b'wa' ... b'he' b'n ' b'th']\n",
      " [b'it' b' s' b'o ' ... b'co' b'ul' b'd ']\n",
      " ...\n",
      " [b'ey' b', ' b'an' ... b'et' b' o' b'ut']\n",
      " [b'ng' b' t' b'ho' ... b'an' b'd ' b'th']\n",
      " [b'er' b'e ' b'wa' ... b'le' b'ss' b'in']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 23:03:45.659251: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 23:03:47.498573: W tensorflow/core/framework/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node Cast_1 defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNIMPLEMENTED:  Cast string to float is not supported\n\t [[{{node Cast_1}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_25777]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 165\u001b[0m\n\u001b[1;32m    158\u001b[0m lm_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    159\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    160\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    161\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, PerplexityMetric()],\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[43mlm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node Cast_1 defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNIMPLEMENTED:  Cast string to float is not supported\n\t [[{{node Cast_1}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_25777]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "# Downloading Data\n",
    "url = \"https://www.cs.cmu.edu/%7Espok/grimmtmp/\"\n",
    "dir_name = \"data\"\n",
    "\n",
    "\n",
    "def download_data(url, filename, download_dir):\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    if not os.path.exists(os.path.join(download_dir, filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir, filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# Number of files and their names to download\n",
    "num_files = 209\n",
    "filenames = [format(i, \"03d\") + \".txt\" for i in range(1, num_files + 1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    download_data(url, fn, dir_name)\n",
    "\n",
    "filenames = [os.path.join(dir_name, f) for f in os.listdir(dir_name)]\n",
    "\n",
    "# Splitting the dataset\n",
    "random_state = 54321\n",
    "\n",
    "train_filenames, test_and_valid_filenames = train_test_split(\n",
    "    filenames, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "valid_filenames, test_filenames = train_test_split(\n",
    "    test_and_valid_filenames, test_size=0.5, random_state=random_state\n",
    ")\n",
    "\n",
    "# Generate bigrams and analyze vocabulary\n",
    "bigram_set = set()\n",
    "for fname in train_filenames:\n",
    "    document = []\n",
    "    with open(fname, \"r\") as f:\n",
    "        for row in f:\n",
    "            document.append(row.lower())\n",
    "        document = \" \".join(document)\n",
    "        bigram_set.update([document[i : i + 2] for i in range(0, len(document), 2)])\n",
    "n_vocab = len(bigram_set)\n",
    "print(\"Found {} unique bigrams\".format(n_vocab))\n",
    "\n",
    "\n",
    "# Function to generate tf.data dataset\n",
    "def generate_tf_dataset(filenames, ngram_width, window_size, batch_size, shuffle=False):\n",
    "    documents = []\n",
    "    for f in filenames:\n",
    "        doc = tf.io.read_file(f)\n",
    "        doc = tf.strings.ngrams(\n",
    "            tf.strings.bytes_split(\n",
    "                tf.strings.regex_replace(tf.strings.lower(doc), \"\\n\", \" \")\n",
    "            ),\n",
    "            ngram_width,\n",
    "            separator=\"\",\n",
    "        )\n",
    "        documents.append(doc.numpy().tolist())\n",
    "    documents = tf.ragged.constant(documents)\n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)\n",
    "    doc_dataset = doc_dataset.map(lambda x: x[::ngram_width])\n",
    "    doc_dataset = doc_dataset.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(x)\n",
    "        .window(size=window_size + 1, shift=int(window_size * 0.75))\n",
    "        .flat_map(lambda window: window.batch(window_size + 1, drop_remainder=True))\n",
    "    )\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "    doc_dataset = doc_dataset.batch(batch_size=batch_size)\n",
    "    if shuffle:\n",
    "        doc_dataset = doc_dataset.shuffle(buffer_size=batch_size * 10)\n",
    "    return doc_dataset\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "ngram_length = 2\n",
    "batch_size = 256\n",
    "window_size = 128\n",
    "\n",
    "train_ds = generate_tf_dataset(\n",
    "    train_filenames, ngram_length, window_size, batch_size, shuffle=True\n",
    ")\n",
    "valid_ds = generate_tf_dataset(valid_filenames, ngram_length, window_size, batch_size)\n",
    "test_ds = generate_tf_dataset(test_filenames, ngram_length, window_size, batch_size)\n",
    "\n",
    "# Adapt the TextVectorization layer\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=n_vocab,\n",
    "    standardize=None,\n",
    "    split=None,\n",
    "    input_shape=(window_size,),\n",
    ")\n",
    "\n",
    "raw_train_data = []\n",
    "for f in train_filenames:\n",
    "    with open(f, \"r\") as file:\n",
    "        raw_train_data.append(file.read().lower())\n",
    "\n",
    "text_vectorizer.adapt(raw_train_data)\n",
    "\n",
    "# Apply TextVectorization to dataset\n",
    "train_ds = train_ds.map(lambda x, y: (text_vectorizer(x), y))\n",
    "valid_ds = valid_ds.map(lambda x, y: (text_vectorizer(x), y))\n",
    "\n",
    "# Check data before training\n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    print(\"Input (x):\", x_batch.numpy())\n",
    "    print(\"Target (y):\", y_batch.numpy())\n",
    "\n",
    "# Define LSTM model\n",
    "K.clear_session()\n",
    "\n",
    "lm_model = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.Embedding(n_vocab + 2, 96),\n",
    "        layers.LSTM(512, return_state=False, return_sequences=True),\n",
    "        layers.LSTM(256, return_state=False, return_sequences=True),\n",
    "        layers.Dense(1024, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(n_vocab, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lm_model.summary()\n",
    "\n",
    "\n",
    "# Perplexity Metric\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, name=\"perplexity\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction=\"none\"\n",
    "        )\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "        loss_ = self.cross_entropy(real, pred)\n",
    "        step1 = K.mean(loss_, axis=-1)\n",
    "        perplexity = K.exp(step1)\n",
    "        return perplexity\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "        super().update_state(perplexity)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "lm_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\", PerplexityMetric()],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lm_model.fit(train_ds, validation_data=valid_ds, epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# About the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 files found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "url = \"https://www.cs.cmu.edu/%7Espok/grimmtmp/\"\n",
    "dir_name = \"data\"\n",
    "\n",
    "\n",
    "def download_data(url, filename, download_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right sizze\"\"\"\n",
    "\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(os.path.join(download_dir, filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir, filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# Number of files and their names to download\n",
    "num_files = 209\n",
    "filenames = [format(i, \"03d\") + \".txt\" for i in range(1, num_files + 1)]\n",
    "\n",
    "for fn in filenames:\n",
    "    download_data(url, fn, dir_name)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print(\"{} files found.\".format(len(filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the little hen went with the little cock\n",
      "to the nut-hill, and they agreed together that whichsoever of\n",
      "them found a kernel of a nut should share it with the other.\n",
      "Then the hen found a large, large nut, but said nothing about\n",
      "it, intending to eat the kernel herself.  The kernel, however,\n",
      "was so large that she could not swallow it, and it remained\n",
      "sticking in her throat, so that she was alarmed lest she should\n",
      "be choked.  Then she cried, cock, I entreat you to run as fast\n",
      "as you can and fetch me some water, or I shall choke.  The little\n",
      "cock did run as fast as he could to the spring, and said, stream,\n",
      "you are to give me some water, the little hen is lying on the\n",
      "nut-hill, and she has swallowed a large nut, and is choking.  The\n",
      "well answered, first run to the bride, and get her to give you\n",
      "some red silk.  The little cock ran to the bride and said, bride,\n",
      "you are to give me some red silk, I want to give red silk to the\n",
      "well, the well is to give me some water, I am to take the water\n",
      "to the little hen who is lying on the nut-hill and has\n",
      "swallowed a great nut-kernel, and is choking with it.  The bride\n",
      "answered, first run and bring me my little wreath which is\n",
      "hanging to a willow.  So the little cock ran to the willow, and\n",
      "drew the wreath from the branch and took it to the bride, and\n",
      "the bride gave him some red silk for it, which he took to the\n",
      "well, who gave him some water for it.  Then the little cock took\n",
      "the water to the hen, but when he got there the hen had choked in\n",
      "the meantime, and lay there dead and did not move.  Then the\n",
      "cock was so distressed that he cried aloud, and every animal\n",
      "came to lament the little hen, and six mice built a little\n",
      "carriage to carry her to her grave, and when the carriage was\n",
      "ready they harnessed themselves to it, and the cock drove.  On\n",
      "the way, however, they met the fox, who said, where are you\n",
      "going, little cock.  I am going to bury my little hen.  May\n",
      "I drive with you.  Yes, but seat yourself at the back of the\n",
      "carriage, for in the front my little horses could not drag you.\n",
      "Then the procession went onwards, and they reached a stream.\n",
      "How are we to cross over, said the little cock.  A straw was\n",
      "lying by the stream and it said, I will lay myself straight\n",
      "across, and then you\n",
      "can drive over me.  But when the six mice came to the bridge,\n",
      "the straw slipped and fell into the water, and the six mice\n",
      "all fell in and were drowned.  Then they were again in difficulty,\n",
      "and a coal came and said, I am large enough, I will lay myself\n",
      "across, and you shall drive over me.  So the coal also laid\n",
      "itself across the water, but unhappily just touched it, at which\n",
      "the coal hissed, was extinguished and died.  When a stone saw\n",
      "that, it took pity on the little cock, wished to help him, and\n",
      "laid itself over the water.  Then the cock drew the carriage\n",
      "himself, but when he got it over and reached the shore with the\n",
      "dead hen, and was about to draw over the others who were sitting\n",
      "behind as well, there were too many of them, the carriage ran\n",
      "back, and they all fell into the water together, and were drowned.\n",
      "Then the little cock was left alone with the dead hen, and dug a\n",
      "grave for her and laid her in it, and made a mound above it, on\n",
      "which he sat down and fretted until he died too, and then everyone\n",
      "was dead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_file = random.choice(os.listdir(\"./data\"))\n",
    "random_file\n",
    "\n",
    "f = open(f\"./data/{random_file}\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating training, validation and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 167 files in the train dataset (e.g.['data/199.txt', 'data/036.txt', 'data/043.txt'])\n",
      "Got 21 files in the valid dataset (e.g.['data/088.txt', 'data/118.txt', 'data/016.txt'])\n",
      "Got 21 files in the test dataset (e.g.['data/176.txt', 'data/195.txt', 'data/123.txt'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fix the random seed so we get the same output everytime\n",
    "random_state = 54321\n",
    "\n",
    "filenames = [os.path.join(dir_name, f) for f in os.listdir(dir_name)]\n",
    "\n",
    "# First separate train and valid+test data\n",
    "train_filenames, test_and_valid_filenames = train_test_split(\n",
    "    filenames, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "# Separate valid+test data to validation and test data\n",
    "valid_filenames, test_filenames = train_test_split(\n",
    "    test_and_valid_filenames, test_size=0.5, random_state=random_state\n",
    ")\n",
    "\n",
    "for subset_id, subset in zip(\n",
    "    (\"train\", \"valid\", \"test\"), (train_filenames, valid_filenames, test_filenames)\n",
    "):\n",
    "    print(\n",
    "        \"Got {} files in the {} dataset (e.g.{})\".format(\n",
    "            len(subset), subset_id, subset[:3]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the vocabulary size.\n",
    "\n",
    "I will be using bigrams to train the language model. The story will be split into units of two characters.\n",
    "The characters will be converted to lowercase to reduce the input dimensionality. Using character-level bigrams helps to reduce the vocabulary, leading to faster model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 696 unique bigrams\n"
     ]
    }
   ],
   "source": [
    "bigram_set = set()\n",
    "# Go through each file in the training set\n",
    "for fname in train_filenames:\n",
    "    document = []  # This will hold all the text\n",
    "    with open(fname, \"r\") as f:\n",
    "        for row in f:\n",
    "            # Convert text to lower case to reduce input dimensionality\n",
    "            document.append(row.lower())\n",
    "            # From the list of text we have, generate one long string\n",
    "            # (containing all training stories)\n",
    "        document = \" \".join(document)\n",
    "        # Update the set with all bigrams found\n",
    "        bigram_set.update([document[i : i + 2] for i in range(0, len(document), 2)])\n",
    "# Assign to a variable and print\n",
    "n_vocab = len(bigram_set)\n",
    "print(\"Found {} unique bigrams\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining th etf.data pipeline\n",
    "\n",
    "#### function generate_tf_dataset() that takes </br>\n",
    "\n",
    "1. filenames - as list of filenames containing the text to be used for the model\n",
    "2. ngram_width - widht of the n-grams to be extracted\n",
    "3. window_size - lenght of the sequence of n-grams to be used to generate a single data point for the model\n",
    "4. batch_size - Size of the batch\n",
    "5. shuffle - (default to False ) whether to shuffle the data or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 22:58:48.801943: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-11 22:58:48.812008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-11 22:58:48.824173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-11 22:58:48.827675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-11 22:58:48.836418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 22:58:49.856323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_tf_dataset(filenames, ngram_width, window_size, batch_size, shuffle=False):\n",
    "    \"\"\"Generate batched data from a list of files specified\"\"\"\n",
    "    # Read the data found in the documents\n",
    "    documents = []\n",
    "    for f in filenames:\n",
    "        doc = tf.io.read_file(f)\n",
    "        doc = tf.strings.ngrams(  # Generate ngrams from the string\n",
    "            tf.strings.bytes_split(\n",
    "                # Create a list of chars from a string\n",
    "                tf.strings.regex_replace(\n",
    "                    # Replace new lines with space\n",
    "                    tf.strings.lower(doc),  # Convert string to lower case\n",
    "                    \"\\n\",\n",
    "                    \" \",\n",
    "                )\n",
    "            ),\n",
    "            ngram_width,\n",
    "            separator=\"\",\n",
    "        )\n",
    "    documents.append(doc.numpy().tolist())\n",
    "    # documents is a list of list of strings, where each string is a story\n",
    "    # From that we generate a ragged tensor\n",
    "    documents = tf.ragged.constant(documents)\n",
    "\n",
    "    # Create a dataset where each row in the ragged tensor would be a sample\n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)\n",
    "\n",
    "    # perform a quick transformation -tf.strings.ngrams\n",
    "    # would generate all the ngrams (e.g. abcd => ab, bc, cd) with\n",
    "    # overlaop, however for our data we do not need the overlap, so we need to skip the overlapping ngrams\n",
    "\n",
    "    doc_dataset = doc_dataset.map(lambda x: x[::ngram_width])\n",
    "\n",
    "    # using window function to generate windows from text\n",
    "    # for a text sequence with window_size 3 and shift 1, you get e.g ab, cd, ef, gh, ij,... =>  [ab, cd, ef], [cd, ef, gh], [ef, gh, ij],...\n",
    "    # each of these windows is a single training sequence for the model\n",
    "\n",
    "    doc_dataset = doc_dataset.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(x)\n",
    "        .window(size=window_size + 1, shift=int(window_size * 0.75))\n",
    "        .flat_map(lambda window: window.batch(window_size + 1, drop_remainder=True))\n",
    "    )\n",
    "\n",
    "    # From each windowed sequence we generate input and target tuple\n",
    "    # e.g. [ab, cd, ef] -> ([ab, cd], [cd, ef])\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "\n",
    "    # Batch the data\n",
    "    doc_dataset = doc_dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Shuffle the data if required\n",
    "    doc_dataset = (\n",
    "        doc_dataset.shuffle(buffer_size=batch_size * 10) if shuffle else doc_dataset\n",
    "    )\n",
    "\n",
    "    # return data\n",
    "    return doc_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728701931.546171   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728701931.581930   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728701931.581981   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728701931.585887   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728701931.585942   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728701931.585962   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728701931.778748   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728701931.778822   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-11 22:58:51.778830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728701931.778864   52320 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-11 22:58:51.778883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "ngram_length = 2\n",
    "batch_size = 256\n",
    "window_size = 128\n",
    "\n",
    "train_ds = generate_tf_dataset(\n",
    "    train_filenames, ngram_length, window_size, batch_size, shuffle=True\n",
    ")\n",
    "valid_ds = generate_tf_dataset(valid_filenames, ngram_length, window_size, batch_size)\n",
    "\n",
    "test_ds = generate_tf_dataset(test_filenames, ngram_length, window_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'th' b'er' b'e ' b'wa' b's ' b'on' b'ce' b' a' b'n ' b'en']] => [[b'er' b'e ' b'wa' b's ' b'on' b'ce' b' a' b'n ' b'en' b'ch']]\n",
      "[[b' a' b'n ' b'en' b'ch' b'an' b'tr' b'es' b's,' b' w' b'ho']] => [[b'n ' b'en' b'ch' b'an' b'tr' b'es' b's,' b' w' b'ho' b' h']]\n",
      "[[b's,' b' w' b'ho' b' h' b'ad' b' t' b'hr' b'ee' b' s' b'on']] => [[b' w' b'ho' b' h' b'ad' b' t' b'hr' b'ee' b' s' b'on' b's ']]\n",
      "[[b'ee' b' s' b'on' b's ' b'wh' b'o ' b'lo' b've' b'd ' b'ea']] => [[b' s' b'on' b's ' b'wh' b'o ' b'lo' b've' b'd ' b'ea' b'ch']]\n",
      "[[b've' b'd ' b'ea' b'ch' b' o' b'th' b'er' b' a' b's ' b'br']] => [[b'd ' b'ea' b'ch' b' o' b'th' b'er' b' a' b's ' b'br' b'ot']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 22:58:55.487380: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# looking at some data generated by the function\n",
    "\n",
    "ds = generate_tf_dataset(train_filenames, 2, window_size=10, batch_size=1).take(5)\n",
    "\n",
    "for record in ds:\n",
    "    print(record[0].numpy(), \"=>\", record[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the language model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the TExtVectorization layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:340: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(name=name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "\n",
    "# The vectorization layer that will convert string bigrams to IDs\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=n_vocab,\n",
    "    standardize=None,\n",
    "    split=None,\n",
    "    input_shape=(window_size,),\n",
    "    # output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 22:58:55.577767: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'e ', 'th', 'he', ' t', 't ', ' w', ' a', 'd ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.adapt(train_ds)\n",
    "text_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x, y: (x, text_vectorizer(y)))\n",
    "valid_ds = valid_ds.map(lambda x, y: (x, text_vectorizer(y)))\n",
    "\n",
    "\n",
    "# train_ds = train_ds.map(lambda x, y: (text_vectorizer(x), text_vectorizer(y)))\n",
    "# valid_ds = valid_ds.map(lambda x, y: (text_vectorizer(x), text_vectorizer(y)))\n",
    "# test_ds = test_ds.map(lambda x, y: (text_vectorizer(x), text_vectorizer(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the LSTM model. The model will have\n",
    "\n",
    "1. The previously trained TextVectorization layer\n",
    "2. An embedding layer randomly initialized and jointly trained with the model\n",
    "3. Two LSTM layers each with 512 and 256 nodes respectively\n",
    "4. A fully-connected hidden lyaer with 1024 nodes and ReLU activation function\n",
    "5. The final prediction layer with n_vocab nodes and softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,247,232</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">696</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">713,400</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │        \u001b[38;5;34m67,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m1,247,232\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m787,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │       \u001b[38;5;34m263,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m696\u001b[0m)      │       \u001b[38;5;34m713,400\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,078,264</span> (11.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,078,264\u001b[0m (11.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,078,264</span> (11.74 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,078,264\u001b[0m (11.74 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "lm_model = tf.keras.Sequential(\n",
    "    [\n",
    "        text_vectorizer,\n",
    "        layers.Embedding(n_vocab + 2, 96),\n",
    "        layers.LSTM(512, return_state=False, return_sequences=True),\n",
    "        layers.LSTM(256, return_state=False, return_sequences=True),\n",
    "        layers.Dense(1024, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(n_vocab, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining metrics and compiling the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Perplexity\n",
    "\n",
    "\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, name=\"perplexity\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction=\"none\"\n",
    "        )\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "        # The next 4 lines zero-out the padding from loss\n",
    "        # calculations, this follows the logic from:\n",
    "        # https://www.tensorflow.org/beta/tutorials/text/transformer#loss_\n",
    "        # and_metrics\n",
    "        loss_ = self.cross_entropy(real, pred)\n",
    "        # Calculating the perplexity steps:\n",
    "        step1 = K.mean(loss_, axis=-1)\n",
    "        perplexity = K.exp(step1)\n",
    "        return perplexity\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "        super().update_state(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model using\n",
    "\n",
    "1. Sparse categorical cross-entropy as loss function\n",
    "2. Adam as the optimizer\n",
    "3. Accuracy and perplexity as metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'th' b'er' b'e ' ... b'ro' b'ck' b'y ']\n",
      " [b'he' b' e' b'ld' ... b'ut' b'ed' b' u']\n",
      " [b'se' b'a,' b' a' ... b'ps' b', ' b'or']\n",
      " ...\n",
      " [b' h' b'ad' b' d' ... b'nh' b'ur' b't.']\n",
      " [b'd ' b'wi' b'th' ... b's ' b'yo' b'u ']\n",
      " [b'ou' b' a' b're' ... b' b' b'ea' b'ut']], shape=(28, 128), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "lm_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\", PerplexityMetric()],\n",
    ")\n",
    "\n",
    "for i, label in train_ds.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 22:58:57.847722: W tensorflow/core/framework/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_1/Cast defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNIMPLEMENTED:  Cast string to float is not supported\n\t [[{{node sequential_1/Cast}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_43084]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node sequential_1/Cast defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNIMPLEMENTED:  Cast string to float is not supported\n\t [[{{node sequential_1/Cast}}]]\n  (1) CANCELLED:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_43084]"
     ]
    }
   ],
   "source": [
    "lm_model.fit(train_ds, validation_data=valid_ds, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_model.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
